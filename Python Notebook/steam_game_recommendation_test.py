# -*- coding: utf-8 -*-
"""Steam_Game_Recommendation_test.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1u0EdSn2bH-bM2TYyj2Niz6_y_kNIs_D4
"""

pip install pyspark

from google.colab import drive
drive.mount('/content/drive')

"""# Import Libraries and Spark Session

"""

import pandas as pd
import io
import matplotlib.pyplot as plt
from pyspark.rdd import RDD
from pyspark.sql import Row
from pyspark.sql import DataFrame
from pyspark.sql import SparkSession
from pyspark.sql.functions import lit
from pyspark.sql.functions import desc
from pyspark.ml.evaluation import RegressionEvaluator
from pyspark.ml.recommendation import ALS
from pyspark.sql.functions import sum,avg,max,min,mean,count,sqrt
from pyspark.sql.functions import col,when
from pyspark.sql import functions as F 
from pyspark.sql.functions import col,isnan, when, count
from pyspark.ml.evaluation import RegressionEvaluator
import seaborn as sns

#Initialize a spark session.
def spark_intialization():
    spark = SparkSession \
        .builder \
        .appName("Pyspark Project") \
        .config("spark.sql.pivotMaxValues", "200000") \
        .config("spark.driver.memory", "4g") \
        .config("spark.executor.memory", "8g") \
        .getOrCreate()
    return spark

# Initialise spark object
spark = spark_intialization()
spark

"""# DATA PREPROCESSING"""

from pyspark.sql import functions as pyspark_functions
from pyspark.sql.types import *
schema = StructType([ \
                     StructField("USER_ID", IntegerType(), True), \
                     StructField("Steam_Game", StringType(), True),\
                     StructField("Behaviour_Name", StringType(), True),\
                    StructField("Hours_played", FloatType(), True)])
dataframes = spark.read.schema(schema).csv("/content/drive/MyDrive/GOOGLE_COLAB/BigData/steam-200k.csv", header=False)
dataframes.show(10)

dataframes = dataframes.withColumnRenamed("_c0","USER_ID").withColumnRenamed("_c1","Steam_Game").withColumnRenamed("_c2","Behaviour_Name").withColumnRenamed("_c3","Hours_played")
dataframes = dataframes.drop("_c4")
dataframes.show(10)

"""**Exploratory Data Analysis**"""

total_records = dataframes.count()
print("Total records in dataframe", total_records)
# Find Count of Null, None, NaN of All DataFrame Columns
from pyspark.sql.functions import col,isnan, when, count
dataframes.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in dataframes.columns]
   ).show(truncate=False)

#checking for duplicate values
#Hours if behavior is play, 1.0 if behavior is purchase
dataframes.groupBy("USER_ID","Steam_Game","Behaviour_Name", "Hours_played").count().filter("count > 1").show(5)
#no of duplicate rows 
print("Duplicate Values Count",dataframes.groupBy("USER_ID","Steam_Game","Behaviour_Name", "Hours_played").count().filter("count > 1").count())
#storing distinct datapoints of our dataset in distinctDF
df = dataframes.distinct()
print("Distinct count: "+str(df.count()))

#counting distinct users and games on the platform.
total_users = df.select(['USER_ID']).distinct().count()
total_games = df.select(['Steam_Game']).distinct().count()
total_hours = df.select(['Hours_played']).distinct().count()
print("Total No of Users  :", total_users)
print("Total No of Games :", total_games)
print("Total No of hours played by user :", total_hours)

def sparsity(df):
    # Count the number of ratings in the rating dataframe
    hours_played = df.select("Hours_played").count()
    
    # Total Number of distinct users and animes
    total_element = total_users*total_games
    #print(total_element)
    # Calculate  % Sparsity of Matrix Fomrula (1-(rating_count)/(total_element))*100
    sparsity = (1-(hours_played)/total_element)*100
    print(" The dataframe is ", "%.2f" % sparsity +"% sparse")
sparsity(df)

#Most number of hours played Games
topGames=df.groupBy("Steam_Game") \
    .agg(count("Hours_played").alias("Hours")) \
    .sort(desc("Hours"))
topGames.show(10)
topGames_pdf = topGames.toPandas()
topGames_pdf = topGames_pdf.head(10)
# plot a bar graph using pandas
ax = topGames_pdf.plot(kind='barh',x='Steam_Game', y='Hours', title="Most Hours Played Game", color=['orange'],rot=0, fontsize=12,figsize=(12, 6))

# Show the chart
plt.show()

#Top paid Games 
topPurchaseGames = df.filter("Behaviour_Name == 'purchase'").groupBy("Steam_Game","Behaviour_Name").count().sort(desc("count"))
topPurchaseGames.show(10)
topPurchaseGames_pdf = topPurchaseGames.toPandas()

topPurchaseGames_pdf = topPurchaseGames_pdf.head(10)
# plot a bar graph using pandas
ax = topPurchaseGames_pdf.plot(kind='barh',x='Steam_Game', y='count', title="Top 10 Paid Games",color=['purple'],rot=0, fontsize=12,figsize=(12, 6))

# Show the chart
plt.show()

#Top played Games 
topPlayedGames = df.filter("Behaviour_Name == 'play'").groupBy("Steam_Game","Behaviour_Name").count().sort(desc("count"))
topPlayedGames.show(10)
topPlayedGames_pdf = topPlayedGames.toPandas()

topPlayedGames_pdf = topPlayedGames_pdf.head(10)
# plot a bar graph using pandas
ax = topPlayedGames_pdf.plot(kind='barh',x='Steam_Game', y='count', title="Top 10 Played Games",color=['pink'],rot=0, fontsize=12,figsize=(12, 6))

# Show the chart
plt.show()

dataframes.select("Hours_played").describe().show()

"""**Outlier Analysis**"""

#Calculate Upper, Lower and InterQuartlie Range
def calculate_bounds(df):
  c = "Hours_played"
  bounds = {
        c: dict(
            zip(["q1", "q3"], df.approxQuantile(c, [0.25, 0.75], 0))
        )
      
    }
  iqr = bounds[c]['q3'] - bounds[c]['q1']
  bounds[c]['min'] = bounds[c]['q1'] - (iqr * 1.5)
  bounds[c]['max'] = bounds[c]['q3'] + (iqr * 1.5)
  return bounds
#Indicate the presence of Outlier in the specic Column
def flag_outliers(df, id_col):
  bounds = calculate_bounds(df)
  outliers = {}
  c = "Hours_played"
  return df.select(c, id_col,
        *[
            F.when(
                ~F.col(c).between(bounds[c]['min'], bounds[c]['max']),
                "yes"
            ).otherwise("no").alias(c+'_outlier')
        ]
  )
#Columns Containing Outliers
outlier_data = flag_outliers(df,'USER_ID')
outlier_data.show(10)

panda_df= df.toPandas()
sns.set()
ax = sns.boxplot(x=panda_df['Hours_played'])

"""# FEATURE ADDITION"""

from pyspark.sql.window import Window
from pyspark.sql.functions import col, when, lag, sum
window_spec = Window.orderBy('USER_ID')
data_with_prev_value = dataframes.withColumn('prev_value', lag(col('Behaviour_Name')).over(window_spec))
combined_data = data_with_prev_value.withColumn('new_feature', when((col('prev_value') == 'purchase') & (col('Behaviour_Name') == 'play'), 2).otherwise(1))
grouped1 = combined_data.filter(((col('prev_value') == 'purchase') & (col('Behaviour_Name') == 'play')) | \
                                        ((col('prev_value') == 'purchase') & (col('Behaviour_Name') == 'purchase')) |\
                                        ((col('prev_value') == 'null') & (col('Behaviour_Name') == 'purchase')) |\
                                        ((col('prev_value') == 'play') & (col('Behaviour_Name') == 'purchase')));
grouped1.show(10)
grouped1.count()

average = grouped1.groupBy("Steam_Game") \
            .agg(mean("Hours_played").alias("mean_Hourplayed")) \
             .select("Steam_Game", "mean_Hourplayed")
grouped = grouped1.join(average, on="Steam_Game", how="inner")
grouped.show(10)

from pyspark.sql.functions import when
newfeature = grouped.withColumn("rating", 
                  when(grouped["Hours_played"] == 1.0 * grouped["mean_Hourplayed"] * grouped["new_feature"], 1)
                  .when(grouped["Hours_played"] >= 0.9 * grouped["mean_Hourplayed"] * grouped["new_feature"], 5)
                   .when((grouped["Hours_played"] >= 0.7 * grouped["mean_Hourplayed"] * grouped["new_feature"]) & (grouped["Hours_played"] < 0.9 * grouped["mean_Hourplayed"]*grouped["new_feature"]), 4)
                   .when((grouped["Hours_played"] >= 0.4 * grouped["mean_Hourplayed"] * grouped["new_feature"]) & (grouped["Hours_played"] < 0.7 * grouped["mean_Hourplayed"]*grouped["new_feature"]), 3)
                   .when((grouped["Hours_played"] >= 0.1 * grouped["mean_Hourplayed"] * grouped["new_feature"]) & (grouped["Hours_played"] < 0.4 * grouped["mean_Hourplayed"]*grouped["new_feature"]), 2)
                   .otherwise(0))
newfeature.show(10)

# datapoints with less than 1 hour played games. We wont consider 16792 datapoints 
new_feature1 =dataframes.filter("Hours_played < 1")
new_feature1.count()

newfeature.groupBy("rating").count().show()

"""## VISUALIZATION"""

heatmap = newfeature.toPandas()
corr = heatmap.corr(method='pearson')
#Heatmap
sns.set(rc={'figure.figsize':(10,5)})
sns.heatmap(corr, square=True, annot=True, cmap='viridis')

trainpd = newfeature.toPandas()
no_of_ratings_per_game = trainpd.groupby(by='USER_ID')['rating'].count().sort_values(ascending=False)

fig = plt.figure(figsize = (12, 6))
ax = plt.gca()
plt.plot(no_of_ratings_per_game.values)
plt.title('# Ratings/Game')
plt.xlabel('Games')
plt.ylabel('No of Users who rated Games')
ax.set_xticklabels([])
plt.show()

# It is very skewed, just like the number of ratings given per user.
# There are some Games (which are very popular) which are rated by huge number of users.
# But most of the Games(like 90%) got some 0-100 of user ratings only.

"""Vishanth Model Implementation"""

newfeature.count()

"""**ALS IMPLEMENTATION**"""

from pyspark.ml.feature import VectorAssembler
from pyspark.ml.feature import Normalizer
from pyspark.ml.feature import MinMaxScaler
from pyspark.ml.feature import StringIndexer
from pyspark.ml.recommendation import ALS
from pyspark.ml.evaluation import RegressionEvaluator

newfeature.printSchema()
pandasdf = newfeature.toPandas()
pandasdf['Steam_Game'] = pandasdf['Steam_Game'].astype('category')
d = dict(enumerate(pandasdf['Steam_Game'].cat.categories))
pandasdf['GAME_ID'] = pandasdf['Steam_Game'].cat.codes
pandasdf.head(10)

newfeature = spark.createDataFrame(pandasdf)
newfeature.show(10)

train, test = newfeature.randomSplit([0.8, 0.2])

from pyspark.sql.functions import avg
## Calculating user-item interaction
# Calculate User Mean
userMeanDF = train.groupBy('USER_ID').agg({'rating': 'mean'}).withColumnRenamed("avg(rating)", "user_mean")
# Calculate Anime Mean
itemMeanDF = train.groupBy('GAME_ID').agg({'rating': 'mean'}).withColumnRenamed("avg(rating)", "item_mean")
# Calculate Global Average in Train set
globalAverageDF = train.select(avg('rating'))
globalAverage = globalAverageDF.collect()[0]['avg(rating)']
# Adding the mean columns to train DF
meanDF = train.join(userMeanDF, ['USER_ID'], "inner").join(itemMeanDF, ['GAME_ID'], "inner")
#create final DF with user-item interaction column
interactionDF = meanDF.withColumn('user_item_interaction', meanDF['rating']-(meanDF['user_mean'] + meanDF['item_mean'] - globalAverage))
interactionDF.show(5)

interactionDF.drop("Steam_Game","Behaviour_Name","Hours_played","mean_Hourplayed") \
    .printSchema()
als = ALS(maxIter=5, regParam=0.01, userCol="USER_ID", itemCol="GAME_ID", ratingCol="user_item_interaction",
          coldStartStrategy="drop")
model = als.fit(interactionDF)
predictions = model.transform(test)
predictions.show(5)

evaluator = RegressionEvaluator(metricName="rmse", labelCol="rating", predictionCol="prediction")
rmse = evaluator.evaluate(predictions)
print("Root-mean-square error = " + str(rmse))
nrecommendations = model.recommendForAllUsers(5)
nrecommendations.show(10)
import pyspark.sql.functions as F
nrecommendations = nrecommendations.withColumn("rec_exp", F.explode("recommendations"))\
    .select('USER_ID', F.col("rec_exp.GAME_ID"), F.col("rec_exp.rating"))
nrecommendations.show(10)

Games = spark.read.schema(schema).csv("/content/drive/MyDrive/GOOGLE_COLAB/BigData/steam-200k.csv", header=False)
Gamesdf = Games.toPandas()
Gamesdf['Steam_Game'] = Gamesdf['Steam_Game'].astype('category')
Gamesdf['GAME_ID'] = Gamesdf['Steam_Game'].cat.codes
Games = spark.createDataFrame(Gamesdf)
Games = Games.select(F.col("USER_ID"))

# User’s ALS Recommendations:

# Joining recommended movies for the given user_id with anime dataframe to display recommendations in more readable format
int_df = nrecommendations.join(Games, on='USER_ID').filter('user_id = 151603712')
int_df = int_df.distinct()
int_df.count()
int_df.show()
int_df = int_df.toPandas()
d[3881]
int_df = int_df.sort_values(['rating'], ascending=[False])
int_df['Game_Name'] = int_df['GAME_ID'].map(d)
int_df

"""**COSINE SIMILARITY MODEL**"""

# Select only the relevant columns
df = newfeature.select("User_ID", "Steam_Game", "rating")
# Filter out any null values
df = df.filter(df.User_ID.isNotNull() & df.Steam_Game.isNotNull() & df.rating.isNotNull())
from pyspark.sql.functions import col
# Create a pivot table
pivot_table = df.groupBy("Steam_Game").pivot("User_ID").agg(avg(col("rating"))).alias("avg_rating")
# Fill any null values with 0
pivot_table = pivot_table.na.fill(0)

from pyspark.ml.feature import VectorAssembler
from pyspark.ml.feature import Normalizer
from pyspark.ml.linalg import Vectors
from pyspark.ml.feature import BucketedRandomProjectionLSH
# Convert the pivot table to a VectorAssembler
assembler = VectorAssembler(inputCols=pivot_table.columns[1:], outputCol="features")
df_vector = assembler.transform(pivot_table)
# Normalize the features
normalizer = Normalizer(inputCol="features", outputCol="normFeatures")
df_norm = normalizer.transform(df_vector)